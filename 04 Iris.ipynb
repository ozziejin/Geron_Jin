{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "list(iris.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] (150,) ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "print(iris['data'].shape, iris['feature_names'], iris['target'].shape, iris['target_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['target'][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labels are sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "rnd_indices = np.random.permutation(iris['target'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the X and y, and intercept to X\n",
    "X = iris['data'][rnd_indices]\n",
    "y = iris['target'][rnd_indices]\n",
    "X_b = np.c_[np.ones([len(X),1]), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, val, test split\n",
    "X_train, X_val, X_test = X_b[:90], X_b[90:120], X_b[120:]\n",
    "y_train, y_val, y_test = y[:90], y[90:120], y[120:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### categoritize labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "OneHot = OneHotEncoder(sparse=False)\n",
    "y_train_c = OneHot.fit_transform(y_train.reshape(-1,1))\n",
    "y_val_c = OneHot.transform(y_val.reshape(-1,1))\n",
    "y_test_c = OneHot.transform(y_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax score for class k:\n",
    "\n",
    "$s_k(\\mathbf{x})=\\mathbf{x}^T\\mathbf{\\theta}^{(k)}$\n",
    "\n",
    "softmax function:\n",
    "\n",
    "$\\sigma\\left(\\mathbf{s}(\\mathbf{x})\\right)_k = \\dfrac{\\exp\\left(s_k(\\mathbf{x})\\right)}{\\sum\\limits_{j=1}^{K}{\\exp\\left(s_j(\\mathbf{x})\\right)}}$\n",
    "\n",
    "So the equations we will need are the cost function:\n",
    "\n",
    "$J(\\mathbf{\\Theta}) =\n",
    "- \\dfrac{1}{m}\\sum\\limits_{i=1}^{m}\\sum\\limits_{k=1}^{K}{y_k^{(i)}\\log\\left(\\hat{p}_k^{(i)}\\right)}$\n",
    "\n",
    "And the equation for the gradients:\n",
    "\n",
    "$\\nabla_{\\mathbf{\\theta}^{(k)}} \\, J(\\mathbf{\\Theta}) = \\dfrac{1}{m} \\sum\\limits_{i=1}^{m}{ \\left ( \\hat{p}^{(i)}_k - y_k^{(i)} \\right ) \\mathbf{x}^{(i)}}$\n",
    "\n",
    "Note that $\\log\\left(\\hat{p}_k^{(i)}\\right)$ may not be computable if $\\hat{p}_k^{(i)} = 0$. So we will add a tiny value $\\epsilon$ to $\\log\\left(\\hat{p}_k^{(i)}\\right)$ to avoid getting `nan` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.116997060758296\n",
      "500 0.42627276322546365\n",
      "1000 0.3514195911116283\n",
      "1500 0.3067304355407432\n",
      "2000 0.27518666437267236\n",
      "2500 0.25147669365325015\n",
      "3000 0.23295057657761428\n",
      "3500 0.21805514741926596\n",
      "4000 0.2058054253503143\n",
      "4500 0.19554408165849232\n",
      "5000 0.1868152809764123\n"
     ]
    }
   ],
   "source": [
    "eta = 0.01           # learning rate\n",
    "m = len(X_train)     # instance rows\n",
    "epsilon = 10**(-7)  # small positive number to avoid nan\n",
    "n_iterations = 5001\n",
    "theta = np.random.randn(X_train.shape[1], y_train_c.shape[1]) # random initialization, theta is alwaays diagonal against X, (5,3)\n",
    "\n",
    "def softmax(logits):\n",
    "    exps = np.exp(logits)\n",
    "    exp_sums = np.sum(exps, axis=1, keepdims=True)\n",
    "    return exps / exp_sums\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    score = X_train.dot(theta)   # probably of each class (90,5)*(5,3)\n",
    "    y_prob = softmax(score)      # (90,3)\n",
    "    loss = -np.mean(np.sum(y_train_c * np.log(y_prob + epsilon), axis=1))   # loss function for logistic regression is still MSE\n",
    "    gradients = 1/m * X_train.T.dot(y_prob - y_train_c)  # (5,90)*(90,3)\n",
    "    theta = theta - eta*gradients\n",
    "    if iteration % 500 == 0:\n",
    "        print(iteration, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.72753952,  0.09550867, -2.07416014],\n",
       "       [-0.15804361,  0.78746446, -0.43524657],\n",
       "       [ 3.21150709,  0.11200907, -1.38785118],\n",
       "       [-1.55811271,  0.36487996,  2.22211255],\n",
       "       [-1.39377266, -1.77050988,  1.1902918 ]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model parameter after 5001 iternations\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score validation set\n",
    "score = X_val.dot(theta)\n",
    "y_prob = softmax(score) # (30*3) matrix of estimated probabilities\n",
    "y_predict = np.argmax(y_prob, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_val)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 9.293419031437885\n",
      "10 1.9247294285507413\n",
      "20 1.9240660895486263\n",
      "30 1.9382071937700376\n",
      "40 1.9602582682608074\n",
      "50 1.9869232579823382\n",
      "60 2.016469219393534\n",
      "70 2.047931385803968\n",
      "80 2.0807478403863757\n",
      "90 2.1145789434351903\n",
      "100 2.1492126568655445\n",
      "110 2.1845124718562237\n",
      "114 1.9218418481984794\n",
      "115 2.2023825566029998 early stopping!\n"
     ]
    }
   ],
   "source": [
    "eta = 0.05           # learning rate\n",
    "m = len(X_train)     # instance rows\n",
    "epsilon = 10**(-7)  # small positive number to avoid nan\n",
    "n_iterations = 5001\n",
    "theta = np.random.randn(X_train.shape[1], y_train_c.shape[1]) # random initialization, theta is alwaays diagonal against X, (5,3)\n",
    "\n",
    "alpha = 0.5  # regularization hyperparameter\n",
    "best_loss = np.infty\n",
    "threshold, consecutive = 100, 0\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    score = X_train.dot(theta)   # probably of each class (90,5)*(5,3)\n",
    "    y_prob = softmax(score)      # (90,3)\n",
    "    xentropy_loss = -np.mean(np.sum(y_train_c * np.log(y_prob + epsilon), axis=1))\n",
    "    l2_loss = 1/2 * np.sum(np.square(theta[1:])) # no need to add loss on bias (intercept)\n",
    "    loss = xentropy_loss + alpha * l2_loss\n",
    "    gradients = 1/m * X_train.T.dot(y_prob - y_train_c)  # (5,90)*(90,3)\n",
    "    theta = theta - eta*gradients\n",
    "    \n",
    "    score = X_val.dot(theta)   # probably of each class (90,5)*(5,3)\n",
    "    y_prob = softmax(score)      # (90,3)\n",
    "    xentropy_loss = -np.mean(np.sum(y_val_c * np.log(y_prob + epsilon), axis=1))\n",
    "    l2_loss = 1/2 * np.sum(np.square(theta[1:])) # use the theta from training\n",
    "    loss = xentropy_loss + alpha * l2_loss\n",
    "    if iteration % 10 == 0:\n",
    "        print(iteration, loss)\n",
    "    if loss < best_loss:\n",
    "        best_loss = loss\n",
    "        consecutive = 0\n",
    "    else:\n",
    "        consecutive += 1\n",
    "    if consecutive >= threshold:\n",
    "        print(iteration - 1, best_loss)\n",
    "        print(iteration, loss, \"early stopping!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score validation set\n",
    "score = X_val.dot(theta)\n",
    "y_prob = softmax(score) # (30*3) matrix of estimated probabilities\n",
    "y_predict = np.argmax(y_prob, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_val)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_test.dot(theta)\n",
    "Y_proba = softmax(logits)\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_test)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
